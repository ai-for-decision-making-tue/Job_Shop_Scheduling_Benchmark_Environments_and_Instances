[test_parameters]
seed = 2024                 # Set random seed for result replication
device = "cpu"              # Device for testing ("cpu" or "cuda")
problem_instance = "/fjsp/1_Brandimarte/Mk02.fjs" # Problem instance for testing
trained_policy = "/saved_models/train_20240314_192906/song_10_5.pt" # Load pretrained policy
sample = false              # sample from policy if true, else use argmax (greedy)
show_gantt = true           # draw ganttchart of found solution
save_gantt = true           # save ganttchart to file
save_results = true         # save results to file
exp_name = ""               # name of the experiment, used for saving results.
                            # If empty (""), the name of the problem instance is used
folder = ""                 # folder to save results, used for saving results.
                            # If empty (""), the results are saved to the current working directory.

[train_parameters]
lr = 0.0002                 # Learning rate
betas = [0.9, 0.999]        # Beta values for Adam optimizer
gamma = 1.0                 # Discount factor
K_epochs = 3                # Number of update epochs
eps_clip = 0.2              # Epsilon for clipping in PPO
A_coeff = 1.0               # Coefficient for actor loss
vf_coeff = 0.5              # Coefficient for value function loss
entropy_coeff = 0.01        # Coefficient for entropy loss
max_iterations = 1000       # Maximum number of iterations
save_timestep = 20          # Time step for saving model checkpoints
update_timestep = 1         # Time step for model updates
minibatch_size = 512        # Mini-batch size for training
parallel_iter = 20          # Number of parallel iterations
viz = false                 # Visualize the training process --> when true: first launch local server with: python -m visdom.server (and set to: online)
viz_name = 'training_visualisation' # Name of visualization env
validation_folder = "/1005/" # Folder for validation data
device = "cpu"              # Device for training ("cpu" or "cuda"

[env_parameters]
num_jobs = 10               # Number of jobs in the environment
num_mas = 5                 # Number of machine agents
batch_size = 20             # Batch size for training
ope_feat_dim = 6            # Dimension of operation features
ma_feat_dim = 3             # Dimension of machine agent features
valid_batch_size = 5        # Batch size for validation
device = "cpu"              # Device for training ("cpu" or "cuda")

[model_parameters]
in_size_ma = 3              # Input size for machine agent
out_size_ma = 8             # Output size for machine agent
in_size_ope = 6             # Input size for operation
out_size_ope = 8            # Output size for operation
hidden_size_ope = 128       # Hidden size for operation model
num_heads = [1, 1]          # Number of attention heads
dropout = 0.0               # Dropout probability
n_latent_actor = 64         # Number of latent units for actor
n_latent_critic = 64        # Number of latent units for critic
n_hidden_actor = 3          # Number of hidden layers for actor
n_hidden_critic = 3         # Number of hidden layers for critic
action_dim = 1              # Dimension of action space
device = "cpu"              # Device for training ("cpu" or "cuda"))